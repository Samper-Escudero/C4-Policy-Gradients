{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4 - Policy gradients methods\n",
    "### REINFORCE method\n",
    "\n",
    "This notebook implements a REINFORCE algorithm. The policy network accepts\n",
    "state vectors as inputs and produces a (discrete) probability distribution over the possible actions.\n",
    "\n",
    "The code and theory is based on Alexander Zai. “Deep Reinforcement Learning in Action MEAP V06”. Manning publications. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import gym\n",
    "from matplotlib import pyplot as plt\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jluissamper/.virtualenvs/pytorch/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test environment\n",
    "def testEnv(env, steps = 200):\n",
    "    env.reset()\n",
    "    for _ in range(steps):\n",
    "        env.render()\n",
    "        env.step(env.action_space.sample()) # take a random action\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jluissamper/.virtualenvs/pytorch/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "# Unconmment to check everything is properly installed\n",
    "# A window with a cartpole randomly moving should appear. \n",
    "# Some python3-related errors might occur after closing the environment (window with the cartpole)\n",
    "# Check whether kernel needs to be restarted \n",
    " testEnv(env, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "class REINFORCE():\n",
    "    def __init__(self, n_in, n_out, n_hidden1, n_hidden2, env, gamma = 0.9):\n",
    "        self.n_in = n_in\n",
    "        self.n_out = n_out\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "        \n",
    "       # self.one_hot_reward = np.ones(arms)\n",
    "\n",
    "        # Neural network model definition \n",
    "        self.model = nn.Sequential(OrderedDict([\n",
    "            ('fc1', nn.Linear(n_in, n_hidden1)),\n",
    "            ('ReLu1', nn.LeakyReLU(inplace = True)),\n",
    "            ('fc2', nn.Linear(n_hidden1, n_hidden2)),\n",
    "            ('ReLu2', nn.LeakyReLU(inplace = True)),\n",
    "            ('fc3', nn.Linear(n_hidden2, n_out)),\n",
    "            ('softmax', nn.Softmax())\n",
    "        ])\n",
    "        )\n",
    "       # self.TNetwork = copy.deepcopy(self.QNetwork) # Target Network\n",
    "       # self.TNetwork.load_state_dict(self.QNetwork.state_dict())                \n",
    "    def updateEnv (self, env):\n",
    "        self.env = env\n",
    "        \n",
    "    def playEpisode(self, eps_max_dur, init_state):\n",
    "        transitions_ = []\n",
    "        preds_ = []\n",
    "        state_ = init_state\n",
    "        for st in range(eps_max_dur): \n",
    "            action_, pred_ = self.policy.step(self.model, state_, self.n_out)\n",
    "            new_state_, reward_ = self.policy.execAction(self.env, action_)\n",
    "            transitions_.append((state_, action_, reward_))\n",
    "            preds_.append(pred_.detach().numpy())\n",
    "            if done:\n",
    "                break\n",
    "            state_ = new_state_\n",
    "\n",
    "        return [transitions_, preds_]\n",
    "        \n",
    "    def resetEnv(self):\n",
    "        self.env.reset()\n",
    "        \n",
    "    class policy():\n",
    "        \n",
    "        def step(model, state, n_outs):\n",
    "            pred = model(torch.from_numpy(state).float())\n",
    "            action = np.random.choice(np.array(range(n_outs)), p = pred.data.numpy())\n",
    "            return action, pred\n",
    "            \n",
    "        def execAction (env, action):\n",
    "            new_state, reward, done, info = env.step(action)\n",
    "            return new_state, reward\n",
    "        \n",
    "        def discount_rewards(rewards, disc_factor=0.99):\n",
    "            # create the discount array using the disc factor and multiply it by the rewards\n",
    "            disc_rewards = torch.pow(disc_factor,torch.arange(len(rewards)).float()) * rewards \n",
    "            # Normalize rewards so as to avoid drift/shift in training\n",
    "            # Add small num to den so as to avoid 0 div\n",
    "            norm_dreward = (disc_rewards - disc_rewards.mean()) / (disc_rewards.std() + 1e-09) \n",
    "            return norm_dreward\n",
    "        \n",
    "        # to train the network:\n",
    "        # 1. Calculate prob of the action taken\n",
    "        # 2. Apply discount factor\n",
    "        # 3. backpropagate and minimize the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0009\n",
    "l1 = 4\n",
    "l2 = 150\n",
    "l3 = 100\n",
    "l4 = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "RLmodel = REINFORCE(l1,l4,l2,l3, env)\n",
    "optimizer = torch.optim.Adam(RLmodel.model.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.1695,  0.3796, -0.3949, -1.1542])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testR = torch.Tensor([4, 3, 2, 1])\n",
    "RLmodel.policy.discount_rewards(testR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(preds, r):\n",
    "    return -1 * torch.sum(r * torch.log(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected np.ndarray (got list)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-133-b514e7c0dd2c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0maction_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m# Store the prediction corresponding to the action taken in each step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mpred_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0mprob_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maction_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected np.ndarray (got list)"
     ]
    }
   ],
   "source": [
    "MAX_DUR = 200\n",
    "MAX_EPISODES = 500 \n",
    "gamma_ = 0.99 # discount factor\n",
    "RLmodel.updateEnv(gym.make(\"CartPole-v0\")) \n",
    "\n",
    "for episode in range(MAX_EPISODES):\n",
    "    RLmodel.resetEnv()\n",
    "    done = False \n",
    "    t = 0\n",
    "    obs = [] \n",
    "    actions = []\n",
    "    score = []\n",
    "    [history, pred_batch] = RLmodel.playEpisode(MAX_DUR, init_state)\n",
    "    ep_len = len(history) \n",
    "    score.append(ep_len)\n",
    "    \n",
    "    # Get rewards of the episode and discount-normalize them \n",
    "    reward_batch = torch.Tensor([r for (s,a,r) in history]).flip(dims=(0,))\n",
    "    disc_returns = RLmodel.policy.discount_rewards(reward_batch, gamma_)\n",
    "    # Get states the system has gone through\n",
    "    #pred_batch = torch.Tensor([pred for (s,a,r,pred) in history])\n",
    "    # Get actions\n",
    "    action_batch = torch.Tensor([a for (s,a,r) in history])\n",
    "    # Store the prediction corresponding to the action taken in each step\n",
    "    pred_batch = torch.from_numpy(pred_batch).float()\n",
    "    prob_batch = pred_batch.gather(dim=1,index=action_batch.long().view(-1,1)).squeeze()\n",
    "    \n",
    "    # backpropagate and update\n",
    "    loss = loss_fn(prob_batch, disc_returns)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
